import aiohttp
import asyncio
import subprocess
import json
import os
from aiohttp import ClientSession
from asyncio import Semaphore


def read_and_filter_file():
    filtered_data = []
    with open("resul/hd", 'r', encoding='utf8') as file:
        for line in file:
            line = line.strip()
            if not line or '#' in line or ',' not in line:
                continue
            name, url = line.split(',', 1)
            filtered_data.append((name, url))
    return filtered_data


async def check_url(name, url, session, semaphore):
    async with semaphore:
        try:
            async with session.get(url, timeout=1) as response:
                if response.status == 200:
                    return name, url
        except Exception as e:
            pass
        return None


async def check_urls(url_list, concurrency=50):
    semaphore = Semaphore(concurrency)
    async with ClientSession() as session:
        tasks = [check_url(name, url, session, semaphore) for name, url in url_list]
        valid_urls = await asyncio.gather(*tasks)
    return [url for url in valid_urls if url is not None]



def get_resolution(name, url, timeout=15):
    process = None
    try:
        cmd = ['ffprobe', '-print_format', 'json', '-show_streams', '-select_streams', 'v', url]
        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        stdout, stderr = process.communicate(timeout=timeout)
        info = json.loads(stdout.decode())
        width = int(info['streams'][0]['width'])
        height = int(info['streams'][0]['height'])
        if width >= 1920 and height >= 1080:
            return name, url
    except subprocess.TimeoutExpired:
        process.kill()
    except Exception as e:
        pass
    finally:
        if process:
            process.wait()
    return None


async def check_resolutions(valid_urls, concurrency=30):
    loop = asyncio.get_event_loop()
    semaphore = Semaphore(concurrency)
    
    async def wrapper(name, url):
        async with semaphore:
            return await loop.run_in_executor(None, get_resolution, name, url)
    
    tasks = [wrapper(name, url) for name, url in valid_urls]
    high_res_urls = await asyncio.gather(*tasks)
    return [url for url in high_res_urls if url is not None]



def save_valid_data(valid_data):
    # valid_data_sorted = sorted(valid_data, key=lambda x: x[0])  # 按 name 排序
    # lines = [f"{name},{url}" for name, url in valid_data_sorted]

    lines = [f"{name},{url}" for name, url in valid_data]  # 不排序

    with open("resul/hd", 'w', encoding='utf8') as file:
        file.write("\n".join(lines))



async def main():

    filtered_data = read_and_filter_file()

    valid_urls = await check_urls(filtered_data)

    high_res_urls = await check_resolutions(valid_urls)
    
    save_valid_data(high_res_urls)

if __name__ == '__main__':
    asyncio.run(main())


print("已检测完毕")
